[
  {
    "id": 93,
    "title": "Braina: AI agent for Brain Interactions Analysis",
    "leaders": "Andrea Brovelli (https://brovelli.github.io/)\nMatteo Neri (https://twitter.com/matte_blacks)",
    "collaborators": "CEDRE Aix-Marseille University (https://www.univ-amu.fr/fr/public/cedre)",
    "description": "Do you want to learn how to study brain interactions from neural data? This project is for you!\nBring your own data and learn how to use recent tools developed by the BraiNets team of the INT.\nThe aim is to investigate the nature of collective neural interactions from pairwise relations to higher-order interactions, and their role in cognition.\nThe project is open to novice as well as experts. New comers in the field will learn how to use two Python packages:\nFrites toolbox: Framework for Information Theoretical analysis of Electrophysiological data and Statistics (https://brainets.github.io/frites/)\nHOI toolbox: Higher-Order Interactions (https://brainets.github.io/hoi/)\nYou will learn advanced methods from statistics, information theory and network science, combined with efficient optimisation software (JAX). \nExperts will develop new analysis features and work on a new release of HOI, and help other participants.\nAll will be assisted by Braina, an AI agent (hopefully) expert in the analysis of complex neural interactions based on Gemini CLI. Braina should serve as a comprehensive resource for researchers and students interested in applying information-theoretical measures for the analysis of functional interactions from neuroimaging and electrophysiological data, such as fMRI, MEG, EEG, LFP, MUA and spiking multivariate data (more than one signal)!",
    "goals": "Learn methods for brain interaction analysis from neural time series\nTest Braina\nDevelop new features and new release of Frites and HOI packages\nCreate working groups on simular datasets and research question",
    "learning": "Learn how to perform brain interaction analysis from neural time series",
    "repository": "https://github.com/brainets/braina",
    "communication": "https://framateam.org/int-brainets/channels/braina",
    "onboarding": "Documentation can be found here:\nFrites: https://brainets.github.io/frites/\nHOI: https://brainets.github.io/hoi/\nBraina: https://github.com/brainets/braina",
    "data": "Bring your own data. It MUST be a single trial, time-resolved and multivariate dataset (trials, time, channels/rois).\nOr simulate your own data.\nWe can provide simulated data as well.",
    "skills": "- Python coding (basic)",
    "good_first_issues": "1. issue one:  Read the documentation of the Frites toolbox https://brainets.github.io/frites/\n2. issue two:  Read the documentation of the HOI toolbox https://brainets.github.io/hoi/\n3. issue three: Read the documentation and install Braina https://github.com/brainets/braina\n4. issue four: Format your data to be readable by Frites or HOI\n5. issue five: Prepare a research question you would like to address",
    "num_collaborators": "5-7",
    "image": "<img width=\"1248\" height=\"832\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/303c9b8d-27ff-4cc2-a370-7953c7e5bfe7\" />",
    "type": "other",
    "development_status": "0_concept_no_content",
    "topics": "information_theory\nsignal processing\nnetwork science\ncoding\nAI agents",
    "tools": "other",
    "programming_languages": "Python",
    "modalities": "other",
    "git_skills": "1_commit_push",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/93",
    "created_at": "2026-01-19T15:37:14Z",
    "updated_at": "2026-01-21T09:29:21Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 91,
    "title": "AXB-ART: Comparing Machine-Predicted and Perceived Speech Similarity at the Sentence Level",
    "leaders": "Zheng Yuan",
    "collaborators": "Alessandro D'Ausilio, Noel Nguyen, Leonardo Lancia,",
    "description": "The AXB-ART project implements a perceptual experiment to assess speech similarity using the Alternating Reading Task (ART) corpus. We compare machine-predicted similarity (using ECAPA-TDNN speaker embeddings with cosine similarity) against human perceptual judgments collected through an AXB paradigm experiment.\n\nIn the AXB task, participants listen to three audio samples (A, X, B) and determine which of A or B sounds more similar to X. The experiment uses data from French, Italian, and Slovak L2 English speakers, focusing on Solo (baseline) and Interactive conditions to investigate phonetic convergence.\n\nThe project includes both a desktop version (PsychoPy) and a web version, with a dynamic UI system, audio sequence playback management, and comprehensive data logging.",
    "goals": "- Deploy and test the web-based version of the AXB experiment\n- Implement server-side data collection infrastructure\n- Recruit and run pilot participants (target: 5-10 L1-English listeners)\n- Develop data analysis pipeline for comparing machine vs. human similarity judgments\n- Calculate inter-rater reliability metrics\n- Improve documentation for experiment deployment and data analysis\n- Discuss potential extensions and refinements based on pilot results",
    "learning": "- Designing and implementing psychoacoustic experiments with PsychoPy\n- Building web-based experiments with audio playback\n- Working with speech embeddings (ECAPA-TDNN) for similarity computation\n- Data analysis techniques for perceptual studies\n- Comparing objective ML metrics with human perception\n- Statistical analysis of agreement between machine and human judgments\n- Good practices for reproducible research in speech science",
    "repository": "https://github.com/byronthecoder/AXB-ART",
    "communication": "byron.zheng.yuan@outlook.com",
    "onboarding": "[README.md](https://github.com/user-attachments/files/24679543/README.md)\n\n[Participant_Instructions.md](https://github.com/user-attachments/files/24679546/Participant_Instructions.md)",
    "data": "The Alternating Reading Task (ART) corpus: a behavioural data set the the project leader collected featuring speech data from French, Italian, and Slovak L2 English speakers through an interactive reading task. The data set contains 58 speakers with approximately 120h recordings and demographic data.",
    "skills": "- Python programming (50%): PsychoPy, pandas, numpy, scipy\n- Web development (20%): HTML/JavaScript for online experiments\n- Speech processing (15%): Understanding of acoustic features, embeddings\n- Data analysis (15%): Statistical methods, inter-rater reliability",
    "good_first_issues": "1. Test the desktop experiment and provide feedback on instructions clarity\n2. Help implement data visualization for pilot results\n3. Review and improve documentation\n4. Test the web version across different browsers\n5. Assist with data preprocessing scripts",
    "num_collaborators": "5-7",
    "image": "<img width=\"1024\" height=\"1024\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb49afbf-b5cc-4d2e-8191-88157c1f05e3\" />",
    "type": "method_development",
    "development_status": "1_basic structure",
    "topics": "hypothesis_testing",
    "tools": "Jupyter",
    "programming_languages": "Python",
    "modalities": "behavioral",
    "git_skills": "2_branches_PRs",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/91",
    "created_at": "2026-01-16T18:06:13Z",
    "updated_at": "2026-01-17T02:33:41Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 89,
    "title": "Quantifying Neural Communication Across Brain Regions",
    "leaders": "Pietro Bozzo ( pietro.bozzo@univ-amu.fr, @pietrobozzo )\nGianmarco Cafaro ( g.cafaro14@studenti.unisa.it )\nGabriele Casagrande (gabriele.casagrande@univ-amu.fr, @gcasagrande )\nGiacomo Preti ( giacomo.preti@univ-amu.fr, @giapre )",
    "collaborators": "",
    "description": "The goal of this project is to study how interactions between brain regions support memory consolidation. We’ll do so by studying a network of regions well known for its role in the consolidation of spatial memory, namely hippocampus (HPC), medial prefrontal cortex (mPFC), and nucleus reuniens (NR). The interplay between HPC and mPFC supports first the creation of new memories and later their reorganization into stable representations, allowing subsequent retrieval. Due to its anatomical connections, the NR has been hypothesized to act as a mediator in the dialogue of these two regions, and we aim at characterizing this interaction.\n\nThe experimental data we’ll use comes from rats chronically implanted with silicon probes in the aforementioned structures. The animals were recorded while performing a spatial navigation learning task and during sleep pre- and post-learning. The data has already been preprocessed to extract the simultaneous spiking activity of hundreds of single neurons, as well as LFP and its characteristic oscillations (hippocampal ripples, thalamo-cortical spindles).\n\nWe aim to explore and test different metrics, or a combination of complementary metrics (e.g., synchronicity, dynamic functional connectivity, transfer entropy), to find hallmarks of communication in the activity of neurons recorded simultaneously from  multiple areas. \n\nAt this stage, we feel that receiving input from a broader community of researchers would be extremely valuable. We already have several techniques in mind that could be applied to this problem, and we would like to discuss them and refine them through interaction with others. We would also greatly appreciate additional perspectives and suggestions, and we believe that BrainHack could be an excellent environment to push this project forward.",
    "goals": "- Test different metrics to extract information about the functional connection within and between the different recorded areas\n- Find evidence of learning through changes in network activity between pre- and post-task sleep",
    "learning": "- Basic principles of memory consolidation physiology and its experimental study\n- Methods to analyze data coming from high-dimensional point processes\n- Interact in collaborative code environment",
    "repository": "",
    "communication": "no_communication_channel_yet",
    "onboarding": "",
    "data": "- High-density electrophysiology recordings\n- Spike trains\n- LFP",
    "skills": "- Basic programming skills (Python)\n- Brainstorming and collaborative discussion",
    "good_first_issues": "1. issue one:\n2. issue two:",
    "num_collaborators": "more",
    "image": "",
    "type": "method_development",
    "development_status": "0_concept_no_content",
    "topics": "hypothesis_testing",
    "tools": "other",
    "programming_languages": "Python",
    "modalities": "other",
    "git_skills": "1_commit_push",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/89",
    "created_at": "2026-01-16T14:05:00Z",
    "updated_at": "2026-01-19T15:41:07Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 88,
    "title": "One Cell Wonder",
    "leaders": "Jean-Michel Arbona - jean-michel.ARBONA@univ-amu.fr\nThéo Brunet - theo.BRUNET@univ-amu.fr\nMaëlle Spiller - maelle.SPILLER@univ-amu.fr",
    "collaborators": "",
    "description": "As members of the Institute of Developmental Biology of Marseille, we are interested in how a whole organism can emerge from a single cell. Although all cells contain the same genetic material, they differentiate during embryonic development into bone, skin, muscle, and many other cell types. This process is guided by a combination of factors, including gene regulation, interactions with neighboring cells, and chemical signals called morphogens, which form gradients that tell cells what they should become based on their position.\n\nInspired by a cellular automata video (https://www.youtube.com/watch?v=qwJNeq-WABU\n) showing how complex structures such as a lizard can arise from simple local rules, we want to recreate and extend this idea into a more complex system. By adjusting and adding rules, such as multiple cell types and different signals, we aim to explore how simple interactions can generate organized patterns. \n\nUltimately, we would like to apply this framework to model aspects of early brain development, in particular to model neurulation. \n\nWe do not aim to replicate real biology in detail, but rather to explore how far simple, biologically inspired rules can go in producing lifelike structures. We therefore want to experiment with key developmental patterning mechanisms, such as morphogen gradients and HOX-gene–like positional identity, to guide how cells differentiate and organize in space. And, of course, most importantly, we want to have fun exploring what emerges.",
    "goals": "- implement a basic cellular automata grid starting from a single cell \n-  build a live visualization of the growing organism and its cell types \n- add simple local interaction rules that control cell division, and differentiation\n- Introduce and explore morphogen signals that influence cell fate  \n- explore how changing how changing morphogen rules affects neural tissue formation",
    "learning": "- basic principles of developmental biology\n- concept of cellular automata\n- visualization and interactive design\n- collaborative, open science workflows",
    "repository": "https://github.com/BrunetTheo/OneCellWonder/blob/main/README.md",
    "communication": "will be established on-site",
    "onboarding": "",
    "data": "- https://www.youtube.com/watch?v=nLu4n7yNGdk\n- https://www.youtube.com/watch?v=qwJNeq-WABU\n- https://distill.pub/2020/growing-ca/",
    "skills": "- basic python coding skills\n- git basics\nbut do not worry we aren't experts either",
    "good_first_issues": "- https://www.youtube.com/watch?v=nLu4n7yNGdk\n- https://www.youtube.com/watch?v=qwJNeq-WABU\n- https://distill.pub/2020/growing-ca/",
    "num_collaborators": "3-5",
    "image": "![Image](https://github.com/user-attachments/assets/b9e4ab36-8e57-4a43-b910-58f0e6df47fe)",
    "type": "coding_methods",
    "development_status": "0_concept_no_content",
    "topics": "other",
    "tools": "Jupyter",
    "programming_languages": "Python",
    "modalities": "not_applicable",
    "git_skills": "1_commit_push",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/88",
    "created_at": "2026-01-14T11:27:30Z",
    "updated_at": "2026-01-14T13:43:16Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 86,
    "title": "Slam: Surface anaLysis And Modeling",
    "leaders": "Guillaume Auzias @gauzias",
    "collaborators": "A group of great folks from the MeCA team, composed of researchers, PhD students, engineers.",
    "description": "Join the efforts from the members of the MeCA team (https://meca-brain.org/) in developing and improving the wonderful open-source python package SLAM !\n\nSlam is an open source python package dedicated to the representation of neuroanatomical surfaces stemming from MRI data in the form of triangular meshes and to their processing and analysis.\n\n- We initiated, developed, use, share and maintain the SLAM package \n- We take the occasion of brainhacks to continuously improve our package in many ways, such as including new features, but also better code testing, tutorials etc\n- It is also the occasion to know more about this incredible resource, and learn to use it.\n- Our project is special and exciting just because we are a community of special and exciting folks, come to us to confirm!\n- To get started, visit the github repos and doc website of SLAM (links below) and prepare yourself to have fun.",
    "goals": "The goals during for this brainhack will be refined together at the beginning of the hacking session, but our global objective is to make SLAM better in all aspects.",
    "learning": "You might learn everything about SLAM and the MeCA team, and much more!",
    "repository": "https://github.com/brain-slam/slam",
    "communication": "We will communicate orally because we will be physically in the same place, fuck covid19 !",
    "onboarding": "https://brain-slam.github.io/slam/",
    "data": "Toy data included in the repos",
    "skills": "- ready to have fun\n- like to share ideas and views \n- python coding, eventually\n- interest in surface processing for neuroimaging",
    "good_first_issues": "You might have a look at our issues:\nhttps://github.com/brain-slam/slam/issues",
    "num_collaborators": "more",
    "image": "![Image](https://github.com/user-attachments/assets/5e56a774-a210-4292-9cef-121338ec66f4)",
    "type": "method_development",
    "development_status": "2_releases_existing",
    "topics": "other",
    "tools": "other",
    "programming_languages": "Python",
    "modalities": "MRI",
    "git_skills": "1_commit_push",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/86",
    "created_at": "2026-01-06T19:19:51Z",
    "updated_at": "2026-01-14T13:34:13Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 82,
    "title": "How do we really measure shape similarity? Rethinking metrics for complex 2D forms",
    "leaders": "Marieva Vlachou",
    "collaborators": "Jean Blouin",
    "description": "This project is part of my PhD research on the processing of tactile spatial information. In a behavioral experiment, blindfolded participants explored the contour of a textured two-dimensional (2D) shape by tracing it with their index finger. Following this tactile exploration, participants completed two tasks:\n\nShape reproduction: participants were asked to reproduce the perceived shape by drawing it once with their finger.\n\nShape identification: participants were asked to identify the explored shape from a set of fifteen candidate shapes printed on a sheet (see attached figure). These candidates varied systematically in their geometric configuration and curvature, spanning different levels of similarity to the explored “reference” shape.\n\nThe central objective of this project is to quantify the similarity between the reference shape and the shapes either reproduced or selected by participants, by identifying variables that best capture shape similarity. As an initial approach, we employed Procrustes analysis and related geometric alignment methods commonly used in shape analysis to quantify dissimilarity between the reference shape, the reproduced shapes, and the perceived alternatives. However, when applied to this predefined set of shapes, these standard metrics did not yield satisfactory results: in particular, they failed to reproduce the intended similarity ranking of the candidate shapes (from most similar:1 to least similar:14, as illustrated in attached figure).\n\nThe shape data were extracted from video recordings of the tracing and drawing tasks and are represented as two-dimensional coordinate sequences (x–y contours) stored in spreadsheet format. \n\nThe challenge addressed by this project is to demonstrate the limitations of widely used shape similarity metrics when applied in isolation and to explore alternative, more integrative methods capable of capturing global shape similarity in complex or abstract 2D forms.\n\n<img width=\"510\" height=\"721\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4ece6808-1b5a-4114-a63b-831d5fac97bc\" />",
    "goals": "The goal of this project is to identify a metric, or a combination of complementary metrics, that can reliably capture and rank the similarity of the predefined set of 2-D shapes (see attached figure), and subsequently apply this similarity measure to the unknown shapes drawn by participants after the tactile exploration task.\n\n- Review and reproduce baseline shape similarity methods (e.g., Procrustes analysis, Chamfer distance, contour-based metrics) on the predefined shape set.\n- Quantitatively evaluate the limitations of these single-metric approaches by testing whether they recover the known similarity ranking of the predefined shapes.\n- Establish a shared data pipeline for loading, resampling, aligning, and visualizing 2D shape contours (MATLAB or Python).\n- Implement and compare multiple complementary shape descriptors (e.g., geometric ratios, curvature-based measures, frequency-domain descriptors, matching costs).\n- Explore CNN-inspired early-layer feature extraction (e.g., edge and orientation filters applied to rasterized shapes) as an alternative shape representation.\n\n**Expected deliverables**\nA benchmark comparison of single-metric versus multivariate shape similarity approaches.",
    "learning": "",
    "repository": "https://github.com/marievavl/Shape_reconstruction",
    "communication": "https://brainhack.slack.com/archives/C0A5BKT0CBF",
    "onboarding": "",
    "data": "https://github.com/marievavl/Shape_reconstruction",
    "skills": "MATLAB coding\nBrainstorming",
    "good_first_issues": "1. issue one: shape preprocessing and visualization\n2. issue two: reproduce baseline shape similarity metrics",
    "num_collaborators": "1-3",
    "image": "",
    "type": "pipeline_development",
    "development_status": "1_basic structure",
    "topics": "data_visualisation",
    "tools": "other",
    "programming_languages": "Matlab",
    "modalities": "behavioral",
    "git_skills": "0_no_git_skills",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/82",
    "created_at": "2025-12-24T14:46:27Z",
    "updated_at": "2025-12-26T09:21:35Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 79,
    "title": "EyePrep: BIDS-App Eye-Tracking Preprocessing tool",
    "leaders": "Sina Kling (mattermost: @sinakling)\nMartin Szinte (mattermost: @martin_szinte)",
    "collaborators": "",
    "description": "Eye-tracking is essential in cognitive neuroscience research, yet preprocessing workflows remain fragmented, inconsistent, and difficult to reproduce across labs. To address this, we've developed BIDS Extension Proposal 20 (BEP20) and conversion tools (https://github.com/bids-standard/eye2bids) to standardize eye-tracking data formats. We are now aiming at building eyetracking preprocessing tools that make this standard immediately practical and valuable.\nEyePrep is designed as an open-source preprocessing pipeline that serves dual purposes:\n  - Incentivize BIDS adoption by providing immediate value once data is formatted correctly\n  - Automated preprocessing and quality control of eyetracking data",
    "goals": "1: Core Infrastructure (Essential)\n\n*Quality Control System*\n- Implement automated QC metrics (data loss %, blink rate, drift magnitude, SNR)\n- Create before/after comparison visualizations for each preprocessing step\n- Deliverable: Every preprocessing run produces a detailed QC report\n\n*Extension of preprocessing steps*\n- Add to existing structure (blink removal methods, smoothing techniques etc)\n- Command-Line Interface \n- Design intuitive CLI with clear parameter names and helpful error messages\n- Implement parameter validation \n- Create configuration file system (YAML/JSON) for reproducible workflows\n- Add comprehensive logging for debugging and transparency\n- Deliverable: Single command preprocessing: eyeprep /data/bids /data/output --config standard\n\n*Documentation & Tutorials*\n- Write comprehensive \"Getting Started\" guide with real examples\n- Create step-by-step tutorial notebooks for common preprocessing workflows\n- Document all preprocessing parameters: what they do, when to use them, typical ranges\n- Deliverable: Users can go from BIDS data to preprocessed output in <30 minutes\n\n2: User Experience (Important)\n\n*Interactive Dashboard*\n- Build web-based parameter exploration tool (Streamlit)\n- Implement real-time preprocessing preview with parameter sliders\n- Create interactive visualizations (zoomable time series, animated scan paths)\n- Add export functionality for chosen configurations\n- Deliverable: Non-coders can explore preprocessing effects visually",
    "learning": "Participants will learn core signal-processing principles and how to turn a research idea into a usable, community-driven BIDS App for eye-tracking data, from initial concept and foundational code to a robust preprocessing tool.",
    "repository": "https://github.com/sinaklg/eyeprep\nhttps://github.com/bids-standard/eye2bids",
    "communication": "none",
    "onboarding": "",
    "data": "",
    "skills": "Depending on participants this project can include many tasks. Possible skills needed: \n- Python(beginner to advanced) - Add features, fix bugs\n- Data visualization (in python) - Create QC plots and reports\n- Signal processing - Validate preprocessing methods\n- DevOps - CI/CD\n- No coding required - Write documentation, UX design, testing",
    "good_first_issues": "1. issue one: https://github.com/sinaklg/eyeprep/issues/3",
    "num_collaborators": "None",
    "image": "",
    "type": "pipeline_development",
    "development_status": "1_basic structure",
    "topics": "other",
    "tools": "BIDS",
    "programming_languages": "Python",
    "modalities": "eye_tracking",
    "git_skills": "2_branches_PRs",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/79",
    "created_at": "2025-12-18T11:16:14Z",
    "updated_at": "2025-12-22T10:07:08Z",
    "labels": [
      "project",
      "project:approved"
    ]
  }
]