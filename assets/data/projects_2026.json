[
  {
    "id": 86,
    "title": "Slam: Surface anaLysis And Modeling",
    "leaders": "Guillaume Auzias @gauzias",
    "collaborators": "A group of great folks from the MeCA team, composed of researchers, PhD students, engineers.",
    "description": "Join the efforts from the members of the MeCA team (https://meca-brain.org/) in developing and improving the wonderful open-source python package SLAM !\n\nSlam is an open source python package dedicated to the representation of neuroanatomical surfaces stemming from MRI data in the form of triangular meshes and to their processing and analysis.\n\n- We initiated, developed, use, share and maintain the SLAM package \n- We take the occasion of brainhacks to continuously improve our package in many ways, such as including new features, but also better code testing, tutorials etc\n- It is also the occasion to know more about this incredible resource, and learn to use it.\n- Our project is special and exciting just because we are a community of special and exciting folks, come to us to confirm!\n- To get started, visit the github repos and doc website of SLAM (links below) and prepare yourself to have fun.",
    "goals": "The goals during for this brainhack will be refined together at the beginning of the hacking session, but our global objective is to make SLAM better in all aspects.",
    "learning": "You might learn everything about SLAM and the MeCA team, and much more!",
    "repository": "https://github.com/brain-slam/slam",
    "communication": "We will communicate orally because we will be physically in the same place, fuck covid19 !",
    "onboarding": "https://brain-slam.github.io/slam/",
    "data": "Toy data included in the repos",
    "skills": "- ready to have fun\n- like to share ideas and views \n- python coding, eventually\n- interest in surface processing for neuroimaging",
    "good_first_issues": "You might have a look at our issues:\nhttps://github.com/brain-slam/slam/issues",
    "num_collaborators": "more",
    "image": "![Image](https://github.com/user-attachments/assets/5e56a774-a210-4292-9cef-121338ec66f4)",
    "type": "method_development",
    "development_status": "2_releases_existing",
    "topics": "other",
    "tools": "other",
    "programming_languages": "Python",
    "modalities": "MRI",
    "git_skills": "1_commit_push",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/86",
    "created_at": "2026-01-06T19:19:51Z",
    "updated_at": "2026-01-06T19:40:04Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 82,
    "title": "How do we really measure shape similarity? Rethinking metrics for complex 2D forms",
    "leaders": "Marieva Vlachou",
    "collaborators": "Jean Blouin",
    "description": "This project is part of my PhD research on the processing of tactile spatial information. In a behavioral experiment, blindfolded participants explored the contour of a textured two-dimensional (2D) shape by tracing it with their index finger. Following this tactile exploration, participants completed two tasks:\n\nShape reproduction: participants were asked to reproduce the perceived shape by drawing it once with their finger.\n\nShape identification: participants were asked to identify the explored shape from a set of fifteen candidate shapes printed on a sheet (see attached figure). These candidates varied systematically in their geometric configuration and curvature, spanning different levels of similarity to the explored “reference” shape.\n\nThe central objective of this project is to quantify the similarity between the reference shape and the shapes either reproduced or selected by participants, by identifying variables that best capture shape similarity. As an initial approach, we employed Procrustes analysis and related geometric alignment methods commonly used in shape analysis to quantify dissimilarity between the reference shape, the reproduced shapes, and the perceived alternatives. However, when applied to this predefined set of shapes, these standard metrics did not yield satisfactory results: in particular, they failed to reproduce the intended similarity ranking of the candidate shapes (from most similar:1 to least similar:14, as illustrated in attached figure).\n\nThe shape data were extracted from video recordings of the tracing and drawing tasks and are represented as two-dimensional coordinate sequences (x–y contours) stored in spreadsheet format. \n\nThe challenge addressed by this project is to demonstrate the limitations of widely used shape similarity metrics when applied in isolation and to explore alternative, more integrative methods capable of capturing global shape similarity in complex or abstract 2D forms.\n\n<img width=\"510\" height=\"721\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4ece6808-1b5a-4114-a63b-831d5fac97bc\" />",
    "goals": "The goal of this project is to identify a metric, or a combination of complementary metrics, that can reliably capture and rank the similarity of the predefined set of 2-D shapes (see attached figure), and subsequently apply this similarity measure to the unknown shapes drawn by participants after the tactile exploration task.\n\n- Review and reproduce baseline shape similarity methods (e.g., Procrustes analysis, Chamfer distance, contour-based metrics) on the predefined shape set.\n- Quantitatively evaluate the limitations of these single-metric approaches by testing whether they recover the known similarity ranking of the predefined shapes.\n- Establish a shared data pipeline for loading, resampling, aligning, and visualizing 2D shape contours (MATLAB or Python).\n- Implement and compare multiple complementary shape descriptors (e.g., geometric ratios, curvature-based measures, frequency-domain descriptors, matching costs).\n- Explore CNN-inspired early-layer feature extraction (e.g., edge and orientation filters applied to rasterized shapes) as an alternative shape representation.\n\n**Expected deliverables**\nA benchmark comparison of single-metric versus multivariate shape similarity approaches.",
    "learning": "",
    "repository": "https://github.com/marievavl/Shape_reconstruction",
    "communication": "https://brainhack.slack.com/archives/C0A5BKT0CBF",
    "onboarding": "",
    "data": "https://github.com/marievavl/Shape_reconstruction",
    "skills": "MATLAB coding\nBrainstorming",
    "good_first_issues": "1. issue one: shape preprocessing and visualization\n2. issue two: reproduce baseline shape similarity metrics",
    "num_collaborators": "1-3",
    "image": "",
    "type": "pipeline_development",
    "development_status": "1_basic structure",
    "topics": "data_visualisation",
    "tools": "other",
    "programming_languages": "Matlab",
    "modalities": "behavioral",
    "git_skills": "0_no_git_skills",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/82",
    "created_at": "2025-12-24T14:46:27Z",
    "updated_at": "2025-12-26T09:21:35Z",
    "labels": [
      "project",
      "project:approved"
    ]
  },
  {
    "id": 79,
    "title": "EyePrep: BIDS-App Eye-Tracking Preprocessing tool",
    "leaders": "Sina Kling (mattermost: @sinakling)\nMartin Szinte (mattermost: @martin_szinte)",
    "collaborators": "",
    "description": "Eye-tracking is essential in cognitive neuroscience research, yet preprocessing workflows remain fragmented, inconsistent, and difficult to reproduce across labs. To address this, we've developed BIDS Extension Proposal 20 (BEP20) and conversion tools (https://github.com/bids-standard/eye2bids) to standardize eye-tracking data formats. We are now aiming at building eyetracking preprocessing tools that make this standard immediately practical and valuable.\nEyePrep is designed as an open-source preprocessing pipeline that serves dual purposes:\n  - Incentivize BIDS adoption by providing immediate value once data is formatted correctly\n  - Automated preprocessing and quality control of eyetracking data",
    "goals": "1: Core Infrastructure (Essential)\n\n*Quality Control System*\n- Implement automated QC metrics (data loss %, blink rate, drift magnitude, SNR)\n- Create before/after comparison visualizations for each preprocessing step\n- Deliverable: Every preprocessing run produces a detailed QC report\n\n*Extension of preprocessing steps*\n- Add to existing structure (blink removal methods, smoothing techniques etc)\n- Command-Line Interface \n- Design intuitive CLI with clear parameter names and helpful error messages\n- Implement parameter validation \n- Create configuration file system (YAML/JSON) for reproducible workflows\n- Add comprehensive logging for debugging and transparency\n- Deliverable: Single command preprocessing: eyeprep /data/bids /data/output --config standard\n\n*Documentation & Tutorials*\n- Write comprehensive \"Getting Started\" guide with real examples\n- Create step-by-step tutorial notebooks for common preprocessing workflows\n- Document all preprocessing parameters: what they do, when to use them, typical ranges\n- Deliverable: Users can go from BIDS data to preprocessed output in <30 minutes\n\n2: User Experience (Important)\n\n*Interactive Dashboard*\n- Build web-based parameter exploration tool (Streamlit)\n- Implement real-time preprocessing preview with parameter sliders\n- Create interactive visualizations (zoomable time series, animated scan paths)\n- Add export functionality for chosen configurations\n- Deliverable: Non-coders can explore preprocessing effects visually",
    "learning": "Participants will learn core signal-processing principles and how to turn a research idea into a usable, community-driven BIDS App for eye-tracking data, from initial concept and foundational code to a robust preprocessing tool.",
    "repository": "https://github.com/sinaklg/eyeprep\nhttps://github.com/bids-standard/eye2bids",
    "communication": "none",
    "onboarding": "",
    "data": "",
    "skills": "Depending on participants this project can include many tasks. Possible skills needed: \n- Python(beginner to advanced) - Add features, fix bugs\n- Data visualization (in python) - Create QC plots and reports\n- Signal processing - Validate preprocessing methods\n- DevOps - CI/CD\n- No coding required - Write documentation, UX design, testing",
    "good_first_issues": "1. issue one: https://github.com/sinaklg/eyeprep/issues/3",
    "num_collaborators": "None",
    "image": "",
    "type": "pipeline_development",
    "development_status": "1_basic structure",
    "topics": "other",
    "tools": "BIDS",
    "programming_languages": "Python",
    "modalities": "eye_tracking",
    "git_skills": "2_branches_PRs",
    "issue_url": "https://github.com/Brainhack-Marseille/brainhack-marseille.github.io/issues/79",
    "created_at": "2025-12-18T11:16:14Z",
    "updated_at": "2025-12-22T10:07:08Z",
    "labels": [
      "project",
      "project:approved"
    ]
  }
]